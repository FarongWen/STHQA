# Perceptual Quality Assessment for Stylized Digital Human
Stylized portraits hold a significant position in the entertainment industry, but the development of traditionally handcrafted or modeled stylized characters has been hindered by their relatively low efficiency. The speech-driven methods offer a novel avenue for manipulating the mouth shape and expressions of stylized digital humans. Despite the proliferation of driving methods, the quality of many generated talking head (TH) videos remains a concern, impacting user visual experiences. Moreover, existing digital human quality assessment methods are predominantly designed for real human faces.
To address this, this paper presents the Stylized Talking Head Quality Assessment (STHQA), a framework tailored for animating digital humans. This paper first introduces the STHQA dataset, comprising 216 animated facial images, 54 audio clips, and 1667 videos generated by 8 speech-driven models, with subjective annotations for multidimensional quality. Based on this dataset, we propose a no-reference assessment method that captures animation-specific features to predict quality scores. Experimental results demonstrate its superiority over mainstream metrics in aligning with human perception of animated digital humans. 
