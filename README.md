# Perceptual Quality Assessment for Stylized Digital Human
## Abstract
Stylized portraits hold a significant position in the entertainment industry, but the development of traditionally handcrafted or modeled stylized characters has been hindered by their relatively low efficiency. The speech-driven methods offer a novel avenue for manipulating the mouth shape and expressions of stylized digital humans. Despite the proliferation of driving methods, the quality of many generated talking head (TH) videos remains a concern, impacting user visual experiences. Moreover, existing digital human quality assessment methods are predominantly designed for real human faces.
To address this, this paper presents the Stylized Talking Head Quality Assessment (STHQA), a framework tailored for animating digital humans. This paper first introduces the STHQA dataset, comprising 216 animated facial images, 54 audio clips, and 1667 videos generated by 8 speech-driven models, with subjective annotations for multidimensional quality. Based on this dataset, we propose a no-reference assessment method that captures animation-specific features to predict quality scores. Experimental results demonstrate its superiority over mainstream metrics in aligning with human perception of animated digital humans. 

## Dataset

The **Stylized Talking Head Quality Assessment (STHQA)** dataset is a comprehensive resource designed for the perceptual quality evaluation of animated talking head videos generated using speech-driven methods. The dataset provides valuable ground truth subjective ratings across several quality dimensions, serving as a crucial foundation for evaluating the visual and audio-visual quality of stylized digital humans.

### Contents:

* **216 Facial Images**: The dataset includes 216 facial images, which represent six distinct stylized visual themes (Anime, Cartoon, Pixar, Arcane, Comic, and Caricature). Each of these styles showcases a different approach to the creation of digital humans, offering a broad variety of artistic designs and facial features.

* **54 Audio Clips**: A selection of 54 audio clips was carefully chosen from the **Common Voice** dataset, representing diverse genders and ages. These audio clips were used to synchronize speech with facial movements, generating speech-driven animated digital humans.

* **1,667 Videos**: The dataset includes 1,667 videos of animated talking heads. These videos were generated using 8 different speech-driven models, which include:

  * **Aniportrait**
  * **Sadtalker**
  * **Audio2head**
  * **Dreamtalk**
  * **Echomimic**
  * **EDtalk**
  * **Hallo**
  * **Real3D**

### Quality Dimensions:

For each video, subjective quality annotations were collected along four dimensions:

* **Distortion**: The degree of visual artifacts or unnatural distortions in the animation.
* **Head Jitter**: The unnatural jitter or instability in head movement.
* **Lip Synchronization**: The alignment between the lip movements and the spoken audio.
* **Overall Quality**: The general perceptual quality of the video.

### Subjective Rating Procedure:

A large-scale subjective study was conducted, where 45 human evaluators (23 men and 22 women) participated in rating the videos. The evaluation was performed in a controlled laboratory environment using a high-resolution iMac monitor. The ratings were collected following ITU-R BT.500-13 guidelines, ensuring consistent and reliable feedback across evaluators.

### Release Information:

The **STHQA dataset** is publicly available and can be accessed through the following [BaiduDisk](https://github.com/FarongWen/STHQA).

This dataset offers a comprehensive benchmark for future research on quality assessment methods tailored for animated talking heads, especially in the context of stylized digital humans. It provides an essential resource for improving both subjective and objective evaluations of speech-driven animation in digital entertainment.

---

Feel free to make adjustments to match the exact format or further specifics from the original content!

